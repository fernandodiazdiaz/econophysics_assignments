{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Lx1UQrgQ1BNTrKf3g8aQKN9A5qJ1RZI8","timestamp":1741694334221},{"file_id":"1ZM1A_SIlHJDidTv9OSmpiFZvYpUnr9gO","timestamp":1735996441027}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install yfinance==0.2.54\n","!pip install arch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yUn8W3ljOpXH","executionInfo":{"status":"ok","timestamp":1742285432957,"user_tz":-60,"elapsed":5435,"user":{"displayName":"Fer Diaz","userId":"08654874336904417659"}},"outputId":"677e1329-0701-4c52-ab60-2f1a39a564fa"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: yfinance==0.2.54 in /usr/local/lib/python3.11/dist-packages (0.2.54)\n","Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.54) (2.2.2)\n","Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.54) (2.0.2)\n","Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.54) (2.32.3)\n","Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.54) (0.0.11)\n","Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.54) (4.3.6)\n","Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.54) (2025.1)\n","Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.54) (2.4.6)\n","Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.54) (3.17.9)\n","Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.54) (4.13.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance==0.2.54) (2.6)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance==0.2.54) (4.12.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance==0.2.54) (2.8.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance==0.2.54) (2025.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance==0.2.54) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance==0.2.54) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance==0.2.54) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance==0.2.54) (2025.1.31)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance==0.2.54) (1.17.0)\n","Requirement already satisfied: arch in /usr/local/lib/python3.11/dist-packages (7.2.0)\n","Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from arch) (2.0.2)\n","Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.11/dist-packages (from arch) (1.14.1)\n","Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.11/dist-packages (from arch) (2.2.2)\n","Requirement already satisfied: statsmodels>=0.12 in /usr/local/lib/python3.11/dist-packages (from arch) (0.14.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->arch) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->arch) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->arch) (2025.1)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.12->arch) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.12->arch) (24.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4->arch) (1.17.0)\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import yfinance as yf\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","\n","from scipy import signal, stats, optimize\n","from arch import arch_model"],"metadata":{"id":"7UC-4SBVe9Cu","executionInfo":{"status":"ok","timestamp":1742285587765,"user_tz":-60,"elapsed":44,"user":{"displayName":"Fer Diaz","userId":"08654874336904417659"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["In the first three exercises of this exercise sheet, we will use as data the SP500 prices between the 1st of January of 1950 and the 31st of December of 2024, with daily precision."],"metadata":{"id":"kwrS1iwoiqSo"}},{"cell_type":"code","source":["# Download the data\n","df = yf.download('^GSPC',datetime(1950, 1, 1), end=datetime(2024, 12, 31), period=\"1y\", progress=False)['Close']\n","print(df.head())"],"metadata":{"id":"Jdb7q8ahi7mZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741694441544,"user_tz":-60,"elapsed":1654,"user":{"displayName":"Fer Diaz","userId":"08654874336904417659"}},"outputId":"952efb6f-c747-427a-a9dd-97dc511b68d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["YF.download() has changed argument auto_adjust default to True\n","Ticker      ^GSPC\n","Date             \n","1950-01-03  16.66\n","1950-01-04  16.85\n","1950-01-05  16.93\n","1950-01-06  16.98\n","1950-01-09  17.08\n"]}]},{"cell_type":"markdown","source":[" # Distribution of returns\n","\n"," As you saw in theory, the time series of returns differs from that of a white noise in several ways.\n"," - First, its probability distribution is typically leptokurtic, meaning it has fatter tails than a normal distribution, indicating a higher probability of extreme events.\n"," - Second, it shows a finite (albeit short) autocorrelation time, which means that the power spectrum is not flat.\n"," - Third, it shows heteroskadisticity in the volatility. Specifically, it experiences volatility clustering, where periods of high volatility tend to follow each other.\n","\n"," In the first exercise, we will analyze the first aspect, namely the fat tails of the return pdf. A typical distribution to model fat tails is the Pareto distribution, which is a one-sided power law pdf.  \n","$$\n","f(x)=\\begin{cases}\n","\\frac{\\alpha\\sigma^\\alpha}{(x-\\mu)^{\\alpha+1}} \\quad x>\\mu+\\sigma \\\\\n","0  \\quad \\quad  x<\\mu+\\sigma\n","\\end{cases}\n","$$\n","\n"],"metadata":{"id":"QrCEz02OziUo"}},{"cell_type":"markdown","source":["# Exercise 1."],"metadata":{"id":"wdxM83zfeCye"}},{"cell_type":"markdown","source":["\n","1. Compute the log returns of the Close value of the index\n","\n","2. Plot the Probability Distribution Function of the **absolute value** of the returns and fit the tail to a power law (Pareto distribution).\n","\n","**Indication:** *the Pareto method from scipy struggles a lot when finding the lower threshold for $x$. The right way to do this exercise is using specialized packages like powerlaw, which automatically find the optimal value of $x_{min}$. I have computed this value for you, so you can stick to scipy: $x_{min} = 0.0209$. Thus, please* **filter out of the dataframe all the values of the absolute log-returns that are smaller than 0.02**.\n","\n","**Clue:** *Exploit the techniques learnt in the previous assignment (the Pareto random variable is implemented as scipy.stats.pareto)*.\n","\n","3. Which value do you obtain for the power law exponent $\\alpha$?\n","\n","4. Is the second moment (the variance) finite?\n","\n","5. Repeat the analysis analysing separately the positive returns and the negative returns (take the absolute value of the latter). Is there any difference when segregating by sign?\n","\n"],"metadata":{"id":"VW8tjnoohm5d"}},{"cell_type":"markdown","source":["# Power Spectrum of financial time-series\n","\n","The power spectrum of a time series indicates the intensity of the signal for each frequency $\\omega$, it can be computed as the Fourier transform of the time-series' autocovariance, $C(s)$,\n","\n","\\begin{equation}\n","P(\\omega)=\\int_{-\\infty}^{\\infty} C(s) e^{-i \\omega s} d s\n","\\end{equation}\n","\n","In practice, it can be easily computed using the **scipy.signal.periodogram** function (https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.periodogram.html).\n","\n","In the lectures you have seen that the power spectrum of financial series is well described by the functional form\n","\n","\\begin{equation}\n","P(\\omega)\\sim \\omega^{-2}\n","\\end{equation}\n","\n"],"metadata":{"id":"nzRy2QO-eCDG"}},{"cell_type":"markdown","source":["# Exercise 2\n","\n","Compute the power spectrum of the SP500 prices. Plot it in log-log scale and compare it with the theoretical prediction. Interpret what the exponent of the power spectrum tells us about the nature of the stochastic process.\n","\n","\n","**Clue:** *recall the `curve_fit` function we used in the previous assignment. Also, keep in mind that we want to fit the tail of the distribution, so it may be useful to filter out the low frequencies from the fit.*"],"metadata":{"id":"jRb66XG5gyix"}},{"cell_type":"markdown","source":["# Moving average and volatility\n","\n","The moving average and moving volatility are mesures of these quantities over fixed intervals, which are usually named the \"window\" over which the measures are taken. These can be useful for smoothing the data for visual analysis or even for quantitative analysis. The moving average and moving volatility can be defined as follows\n","\n","* Moving average\n","\n","\\begin{equation}\n","\\mu_M(j)=\\frac{1}{M}\\sum_{i=j-M+1}^jX(i)\n","\\end{equation}\n","\n","* Moving volatility\n","\n","\\begin{equation}\n","\\sigma^2_M(j)=\\frac{1}{M-1}\\sum_{i=j-M+1}^j(X(i)-\\mu_M(j))^2\n","\\end{equation}\n","\n","Indeed, we can define the moving measure for any observable $\\mathcal{L}$ as\n","\n","\\begin{equation}\n","\\mathcal{L}_M(j)\\left\\{X\\right\\}=\\sum_{i=j-M+1}^j\\mathcal{L}\\left\\{X(i)\\right\\}\n","\\end{equation}"],"metadata":{"id":"dEgPIiscfngJ"}},{"cell_type":"markdown","source":[" # Exercise 3"],"metadata":{"id":"TEH4_AtEiYFl"}},{"cell_type":"markdown","source":["1. Compute the moving average of the  <u>Close price</u> for the previous low frequency data with different window sizes (e.g. 30, 252, 504). Plot it together with the original time series.\n","\n","**Clue:** *The `pandas.Series.rolling(window).statistic()` method computes the given statistic over the given window size through all the series.*\n","\n","*Example: `df[\"Col1\"].rolling(100).mean()`*\n","\n","2. Compute the moving volatility of the <u>log-returns</u> with different window sizes.\n","\n","3. Compute the spectrogram of the moving volatility of the log-returns for a window of 30 days. Can you conclude something from the spectrogram?\n"],"metadata":{"id":"lQLSecDRf7Xr"}},{"cell_type":"markdown","source":["# The GARCH model\n","\n","**Note:** for this part, we will use more recent data. Please make sure to update the returns dataset by running the following cell:"],"metadata":{"id":"10M4S9wsopRd"}},{"cell_type":"code","source":["start = datetime(2000, 1, 1)\n","end = datetime(2023, 12, 31)\n","prices = yf.download('^GSPC',start = start, end=end, period=\"1y\", progress=False)['Close']\n","returns = np.log(1+prices.pct_change()).dropna()*100  #We have to remove the NaNs for the model to work and we rescale the data for better predictions (otherwise a warning shows up)"],"metadata":{"id":"ljk8t-cqopYs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The GARCH model is a fundamental tool in time series analysis for modeling financial data with *heteroskedasticity*, which means that the variability of the process changes over time. Many financial time series, such as stock returns, exhibit periods of high and low volatility, and models like GARCH help us describe this behavior.\n","\n","GARCH stands for *Generalized Autoregressive Conditional Heteroskedasticity*. Breaking this down:  \n","- **Autoregressive** means that past values influence present ones.  \n","- **Conditional** means that today's volatility depends on past information.  \n","- **Heteroskedasticity** means that the variance changes over time instead of being constant.  \n","\n","The simplest version of this family is the ARCH(p) model, where the variance of the stochastic process is modeled as a function of past squared values:\n","\n","$$\n","\\sigma_j^2 = \\alpha_0 + \\alpha_1 x_{j-1}^2 + \\dots + \\alpha_p x_{j-p}^2\n","$$\n","\n","However, ARCH models often require a large number of terms ($ p $) to capture volatility patterns effectively. To improve this, the GARCH(p,q) model introduces an additional autoregressive component in the variance equation:\n","\n","$$\n","\\sigma_j^2 = \\alpha_0 + \\sum_{i=1}^{p} \\alpha_i x_{j-i}^2 + \\sum_{k=1}^{q} \\beta_k \\sigma_{j-k}^2\n","$$\n","\n","\n","Here, past volatility values $ \\sigma_{j-k}^2 $ also contribute to future volatility, making GARCH models more efficient and widely used in finance. Note that GARCH(p,0) is the same as ARCH(p).\n","\n","\n","To determine the appropriate values of $ p $ and $q $, we use the **Bayesian Information Criterion (BIC)**, which helps us balance model fit and complexity, avoiding overfitting.\n","\n","GARCH models are implemented in Python using the `arch` library. These models are typically stored in objects that must be fitted to data before making predictions. In the assignment, you will use Python to fit a GARCH model to financial data and analyze its performance. Let us see an example of how to fit a GARCH process with $p=4, q=2$.\n","\n","*Terminology alert: the function is called arch, but it actually fits data to arbitrary GARCH models*"],"metadata":{"id":"S36XXcQEdJhF"}},{"cell_type":"code","source":["# The parameter o is used to isolate the effect of negative returns (asymetric innovation), here we always stick to o=0\n","\n","arch_object = arch_model(returns, p=4, o=0, q=2)  # Create a python object with our model\n","\n","fitted_arch = arch_object.fit(disp= False)   # Fit our model by calling the fit method in our class. disp = False removes displaying information about the fitting process.\n","\n","fitted_arch.summary()"],"metadata":{"id":"1pKt-feo3KVi","colab":{"base_uri":"https://localhost:8080/","height":503},"executionInfo":{"status":"ok","timestamp":1741694449551,"user_tz":-60,"elapsed":209,"user":{"displayName":"Fer Diaz","userId":"08654874336904417659"}},"outputId":"205cdd6a-3358-4369-907a-32a18c436ea1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<class 'statsmodels.iolib.summary.Summary'>\n","\"\"\"\n","                     Constant Mean - GARCH Model Results                      \n","==============================================================================\n","Dep. Variable:                  ^GSPC   R-squared:                       0.000\n","Mean Model:             Constant Mean   Adj. R-squared:                  0.000\n","Vol Model:                      GARCH   Log-Likelihood:               -8350.47\n","Distribution:                  Normal   AIC:                           16716.9\n","Method:            Maximum Likelihood   BIC:                           16770.6\n","                                        No. Observations:                 6036\n","Date:                Tue, Mar 11 2025   Df Residuals:                     6035\n","Time:                        12:00:49   Df Model:                            1\n","                                 Mean Model                                 \n","============================================================================\n","                 coef    std err          t      P>|t|      95.0% Conf. Int.\n","----------------------------------------------------------------------------\n","mu             0.0604  1.049e-02      5.753  8.743e-09 [3.981e-02,8.095e-02]\n","                               Volatility Model                              \n","=============================================================================\n","                 coef    std err          t      P>|t|       95.0% Conf. Int.\n","-----------------------------------------------------------------------------\n","omega          0.0462  1.299e-02      3.556  3.765e-04  [2.073e-02,7.166e-02]\n","alpha[1]       0.0830  2.158e-02      3.844  1.210e-04    [4.066e-02,  0.125]\n","alpha[2]       0.1335  2.253e-02      5.926  3.106e-09    [8.935e-02,  0.178]\n","alpha[3]       0.0177  2.855e-02      0.619      0.536 [-3.828e-02,7.364e-02]\n","alpha[4]   1.7039e-16  3.046e-02  5.594e-15      1.000 [-5.970e-02,5.970e-02]\n","beta[1]        0.0968      0.101      0.958      0.338      [ -0.101,  0.295]\n","beta[2]        0.6361  9.662e-02      6.583  4.612e-11      [  0.447,  0.825]\n","=============================================================================\n","\n","Covariance estimator: robust\n","\"\"\""],"text/html":["<table class=\"simpletable\">\n","<caption>Constant Mean - GARCH Model Results</caption>\n","<tr>\n","  <th>Dep. Variable:</th>        <td>^GSPC</td>       <th>  R-squared:         </th>  <td>   0.000</td> \n","</tr>\n","<tr>\n","  <th>Mean Model:</th>       <td>Constant Mean</td>   <th>  Adj. R-squared:    </th>  <td>   0.000</td> \n","</tr>\n","<tr>\n","  <th>Vol Model:</th>            <td>GARCH</td>       <th>  Log-Likelihood:    </th> <td>  -8350.47</td>\n","</tr>\n","<tr>\n","  <th>Distribution:</th>        <td>Normal</td>       <th>  AIC:               </th> <td>   16716.9</td>\n","</tr>\n","<tr>\n","  <th>Method:</th>        <td>Maximum Likelihood</td> <th>  BIC:               </th> <td>   16770.6</td>\n","</tr>\n","<tr>\n","  <th></th>                        <td></td>          <th>  No. Observations:  </th>    <td>6036</td>   \n","</tr>\n","<tr>\n","  <th>Date:</th>           <td>Tue, Mar 11 2025</td>  <th>  Df Residuals:      </th>    <td>6035</td>   \n","</tr>\n","<tr>\n","  <th>Time:</th>               <td>12:00:49</td>      <th>  Df Model:          </th>      <td>1</td>    \n","</tr>\n","</table>\n","<table class=\"simpletable\">\n","<caption>Mean Model</caption>\n","<tr>\n","   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>       <th>P>|t|</th>     <th>95.0% Conf. Int.</th>   \n","</tr>\n","<tr>\n","  <th>mu</th> <td>    0.0604</td> <td>1.049e-02</td> <td>    5.753</td> <td>8.743e-09</td> <td>[3.981e-02,8.095e-02]</td>\n","</tr>\n","</table>\n","<table class=\"simpletable\">\n","<caption>Volatility Model</caption>\n","<tr>\n","      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>       <th>P>|t|</th>      <th>95.0% Conf. Int.</th>   \n","</tr>\n","<tr>\n","  <th>omega</th>    <td>    0.0462</td> <td>1.299e-02</td> <td>    3.556</td> <td>3.765e-04</td>  <td>[2.073e-02,7.166e-02]</td>\n","</tr>\n","<tr>\n","  <th>alpha[1]</th> <td>    0.0830</td> <td>2.158e-02</td> <td>    3.844</td> <td>1.210e-04</td>   <td>[4.066e-02,  0.125]</td> \n","</tr>\n","<tr>\n","  <th>alpha[2]</th> <td>    0.1335</td> <td>2.253e-02</td> <td>    5.926</td> <td>3.106e-09</td>   <td>[8.935e-02,  0.178]</td> \n","</tr>\n","<tr>\n","  <th>alpha[3]</th> <td>    0.0177</td> <td>2.855e-02</td> <td>    0.619</td> <td>    0.536</td> <td>[-3.828e-02,7.364e-02]</td>\n","</tr>\n","<tr>\n","  <th>alpha[4]</th> <td>1.7039e-16</td> <td>3.046e-02</td> <td>5.594e-15</td> <td>    1.000</td> <td>[-5.970e-02,5.970e-02]</td>\n","</tr>\n","<tr>\n","  <th>beta[1]</th>  <td>    0.0968</td> <td>    0.101</td> <td>    0.958</td> <td>    0.338</td>    <td>[ -0.101,  0.295]</td>  \n","</tr>\n","<tr>\n","  <th>beta[2]</th>  <td>    0.6361</td> <td>9.662e-02</td> <td>    6.583</td> <td>4.612e-11</td>    <td>[  0.447,  0.825]</td>  \n","</tr>\n","</table><br/><br/>Covariance estimator: robust"],"text/latex":"\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:} &       ^GSPC        & \\textbf{  R-squared:         } &     0.000   \\\\\n\\textbf{Mean Model:}    &   Constant Mean    & \\textbf{  Adj. R-squared:    } &     0.000   \\\\\n\\textbf{Vol Model:}     &       GARCH        & \\textbf{  Log-Likelihood:    } &   -8350.47  \\\\\n\\textbf{Distribution:}  &       Normal       & \\textbf{  AIC:               } &    16716.9  \\\\\n\\textbf{Method:}        & Maximum Likelihood & \\textbf{  BIC:               } &    16770.6  \\\\\n\\textbf{}               &                    & \\textbf{  No. Observations:  } &    6036     \\\\\n\\textbf{Date:}          &  Tue, Mar 11 2025  & \\textbf{  Df Residuals:      } &    6035     \\\\\n\\textbf{Time:}          &      12:00:49      & \\textbf{  Df Model:          } &     1       \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lccccc}\n            & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{95.0\\% Conf. Int.}  \\\\\n\\midrule\n\\textbf{mu} &       0.0604  &    1.049e-02     &     5.753  &      8.743e-09       &   [3.981e-02,8.095e-02]     \\\\\n                  & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{95.0\\% Conf. Int.}  \\\\\n\\midrule\n\\textbf{omega}    &       0.0462  &    1.299e-02     &     3.556  &      3.765e-04       &   [2.073e-02,7.166e-02]     \\\\\n\\textbf{alpha[1]} &       0.0830  &    2.158e-02     &     3.844  &      1.210e-04       &    [4.066e-02,  0.125]      \\\\\n\\textbf{alpha[2]} &       0.1335  &    2.253e-02     &     5.926  &      3.106e-09       &    [8.935e-02,  0.178]      \\\\\n\\textbf{alpha[3]} &       0.0177  &    2.855e-02     &     0.619  &          0.536       &   [-3.828e-02,7.364e-02]    \\\\\n\\textbf{alpha[4]} &   1.7039e-16  &    3.046e-02     & 5.594e-15  &          1.000       &   [-5.970e-02,5.970e-02]    \\\\\n\\textbf{beta[1]}  &       0.0968  &        0.101     &     0.958  &          0.338       &     [ -0.101,  0.295]       \\\\\n\\textbf{beta[2]}  &       0.6361  &    9.662e-02     &     6.583  &      4.612e-11       &     [  0.447,  0.825]       \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{Constant Mean - GARCH Model Results}\n\\end{center}\n\nCovariance estimator: robust"},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["As you can see, we get a lot of nice information about how the parameters were fitted. Apart from the values of the four $\\alpha_i$, the two $\\beta_i$, and $\\omega$, we also get their standard error, their t statistic, their p-value, and their confidence interval.  Moreover, we get the model R^2, log-likelihood, BIC, etc. All these variables are also stored within fitted_arch. For example, fitted_arch.bic returns the BIC."],"metadata":{"id":"qglH0TuClOtd"}},{"cell_type":"code","source":["fitted_arch.bic"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n57x8hbSl24-","executionInfo":{"status":"ok","timestamp":1741694455516,"user_tz":-60,"elapsed":13,"user":{"displayName":"Fer Diaz","userId":"08654874336904417659"}},"outputId":"c420eb99-04a4-47a4-e657-7debd7f31091"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16770.59206187919"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["(**Optional**: *read about the t-statistic and the p-value. What is the relation between the t-statistic and the t distribution we used in the last assignment? In this example, what conclusions can we get from the values we obtained for t and P>|t|?)*"],"metadata":{"id":"NiR2GR5dt0kp"}},{"cell_type":"markdown","source":["# Exercise 4"],"metadata":{"id":"ieCSH6bD1qqb"}},{"cell_type":"markdown","source":["**1. Determine the values of $p$ and $q$ that optimally fit the data to a GARCH process**\n","\n","Fit the model using different combinations of p and q ($p$ ranging from 1 to 5, $q$ ranging from 0 to 4) and compute their BIC values to determine the optimal value for $p$ and $q$."],"metadata":{"id":"XfY5Z_JLzLqi"}},{"cell_type":"markdown","source":["**2. Forecast using optimal model**\n","\n","Use the optimal model that you obtained to perform a forecast of the volatility one step into the future. Specifically, for every time $t$, forecast the volatility at time $t+1$. Plot the predictions along with the moving volatility with window $T=30$. Do you think the model is performing well?\n","\n","*Indication: the forecast is performed automatically using the forecast method of the fitted model object. The syntax of this method is `forecasts = fitted_arch.forecast(horizon=1, start=0, reindex=True, align=target)`.If you are curious on what the arguments of this function are, check it out at https://arch.readthedocs.io/en/latest/. To extract the variance of the forecast, simply call 'forecasts.variance'.*\n","\n","\n"],"metadata":{"id":"uHkWcFFuzSJ4"}},{"cell_type":"markdown","source":["**3 (Optional). Simulate five realizations of future price dynamics for one year ($N=252$) using the optimal GARCH model**. To simulate future values of the time series, you should call `arch_model.simulate(params_garch, N_steps)`, with `params_garch = fitted_arch.params`. Recall that the GARCH model simulates returns, which have to be converted back to prices. Compare with the real evolution of the SP500 in 2024.\n","\n","*Indication: remember that we scaled the returns in this section by a factor of 100. To compare with real data, we must divide the simulated returns by a factor 100 to cancel out the rescaling.*"],"metadata":{"id":"Q8BiDYQn-MTU"}}]}